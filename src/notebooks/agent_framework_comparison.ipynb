{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Framework and LLM API Comparison\n",
    "\n",
    "This notebook demonstrates and compares various agent frameworks and LLM APIs for the same basic use case.\n",
    "We'll measure performance metrics like:\n",
    "- Time to first token\n",
    "- Overall completion time\n",
    "- Total tokens used\n",
    "- Success rate\n",
    "\n",
    "## Use Case: Customer Research Agent\n",
    "We'll create an agent that researches a company and provides key insights for B2B sales preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport json\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nimport yaml\n\n# Load environment variables\nenv_local_path = '../../.env.local'\nload_dotenv(env_local_path)\n\n# Load model configuration\nconfig_path = Path('../../config.yml')\nwith open(config_path, 'r') as f:\n    config = yaml.safe_load(f)\n\n# Extract model IDs from config\nMODEL_IDS = {\n    'openai': config['models']['openai']['default'],\n    'anthropic': config['models']['anthropic']['default'],\n    'xai': config['models']['xai']['default'],\n    'google': config['models']['google']['default'],\n    'strands': config['models']['strands']['default']\n}\n\nprint(\"Loaded model configuration:\")\nfor provider, model_id in MODEL_IDS.items():\n    print(f\"  {provider}: {model_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Simple metrics tracking\"\"\"\n",
    "    framework: str\n",
    "    model: str\n",
    "    start_time: float\n",
    "    completion_time: float = 0\n",
    "    success: bool = False\n",
    "    error_message: str = \"\"\n",
    "    response_content: str = \"\"\n",
    "    \n",
    "    def total_time(self) -> float:\n",
    "        return self.completion_time - self.start_time\n",
    "\n",
    "# Simple metrics tracker\n",
    "metrics = []\n",
    "\n",
    "def track_performance(framework: str, model: str):\n",
    "    \"\"\"Decorator to track performance metrics\"\"\"\n",
    "    def decorator(func):\n",
    "        async def wrapper():\n",
    "            metric = PerformanceMetrics(framework=framework, model=model, start_time=time.time())\n",
    "            try:\n",
    "                response = await func()\n",
    "                metric.completion_time = time.time()\n",
    "                metric.response_content = response\n",
    "                metric.success = True\n",
    "                metrics.append(metric)\n",
    "                print(f\"‚úÖ {framework} completed in {metric.total_time():.2f}s\\n\")\n",
    "                if not framework == \"Strands Agents\":\n",
    "                    print(response)\n",
    "            except Exception as e:\n",
    "                metric.completion_time = time.time()\n",
    "                metric.error_message = str(e)\n",
    "                metrics.append(metric)\n",
    "                print(f\"‚ùå {framework} failed: {e}\")\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Availability: {'OpenAI': '‚úÖ', 'Anthropic': '‚úÖ', 'XAI': '‚úÖ', 'Google': '‚úÖ'}\n"
     ]
    }
   ],
   "source": [
    "# Check API key availability\n",
    "api_keys = {\n",
    "    'OpenAI': bool(os.getenv('OPENAI_API_KEY')),\n",
    "    'Anthropic': bool(os.getenv('ANTHROPIC_API_KEY')),\n",
    "    'XAI': bool(os.getenv('XAI_API_KEY')),\n",
    "    'Google': bool(os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')),\n",
    "}\n",
    "\n",
    "print(\"API Key Availability:\", {k: \"‚úÖ\" if v else \"‚ùå\" for k, v in api_keys.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common prompt for all frameworks\n",
    "RESEARCH_PROMPT = \"\"\"Research the company \"Glean\" and provide:\n",
    "1. Company overview (3-4 sentences)\n",
    "2. Key products/services\n",
    "3. Recent developments\n",
    "4. B2B sales opportunities\n",
    "\n",
    "Keep it concise.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct OpenAI API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"OpenAI Direct\", MODEL_IDS['openai'])\nasync def test_openai_direct():\n    from openai import AsyncOpenAI\n    \n    async with AsyncOpenAI() as client:\n        response = await client.chat.completions.create(\n            model=MODEL_IDS['openai'],\n            messages=[{\"role\": \"user\", \"content\": RESEARCH_PROMPT}]\n        )\n        return response.choices[0].message.content\n\nawait test_openai_direct()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Anthropic API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"Anthropic Direct\", MODEL_IDS['anthropic'])\nasync def test_anthropic_direct():\n    from anthropic import AsyncAnthropic\n    \n    async with AsyncAnthropic() as client:\n        response = await client.messages.create(\n            model=MODEL_IDS['anthropic'],\n            max_tokens=1000,\n            messages=[{\"role\": \"user\", \"content\": RESEARCH_PROMPT}]\n        )\n        return response.content[0].text\n\nawait test_anthropic_direct()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"LangChain\", MODEL_IDS['openai'])\nasync def test_langchain():\n    from langchain_openai import ChatOpenAI\n    \n    llm = ChatOpenAI(model=MODEL_IDS['openai'])\n    response = await llm.ainvoke(RESEARCH_PROMPT)\n    return response.content\n\nawait test_langchain()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strands Agents Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"Strands Agents\", MODEL_IDS['strands'])\nasync def test_strands_agents():\n    from strands import Agent\n    \n    agent = Agent(system_prompt=\"You are a helpful B2B sales research assistant.\")\n    response = agent(RESEARCH_PROMPT)\n    return str(response.data if hasattr(response, 'data') else response)\n\nawait test_strands_agents()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Direct xAI API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"xAI Direct\", MODEL_IDS['xai'])\nasync def test_xai_direct():\n    from openai import AsyncOpenAI\n    \n    client = AsyncOpenAI(\n        api_key=os.getenv('XAI_API_KEY'),\n        base_url=\"https://api.x.ai/v1\"\n    )\n    \n    response = await client.chat.completions.create(\n        model=MODEL_IDS['xai'],\n        messages=[{\"role\": \"user\", \"content\": RESEARCH_PROMPT}]\n    )\n    return response.choices[0].message.content\n\nawait test_xai_direct()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direct Gemini API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@track_performance(\"Gemini Direct\", MODEL_IDS['google'])\nasync def test_gemini_direct():\n    from google import genai\n    \n    client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY'))\n    response = client.models.generate_content(\n        model=MODEL_IDS['google'],\n        contents=RESEARCH_PROMPT\n    )\n    return response.text\n\nawait test_gemini_direct()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display results\ndf = pd.DataFrame([vars(m) for m in metrics])\ndf['total_time'] = df.apply(lambda x: x['completion_time'] - x['start_time'], axis=1)\n\nprint(\"\\nüìä Performance Results:\")\nprint(df[['framework', 'model', 'success', 'total_time']].to_string(index=False))\n\n# Save results to configured path\nresults_path = Path(config['comparison']['results_path'])\nresults_file = results_path / 'agent_framework_comparison_results.csv'\ndf.to_csv(results_file, index=False)\nprint(f\"\\nüíæ Results saved to: {results_file}\")\n\n# Quick visualization\nif df['success'].any():\n    successful = df[df['success']]\n    successful.plot.bar(x='framework', y='total_time', legend=False)\n    plt.title('Response Time Comparison')\n    plt.ylabel('Time (seconds)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}